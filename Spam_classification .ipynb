{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Spam Classification using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a machine learning model which classifies emails as spam or ham(not spam). Email Classification is one of most basic yet important part of machine learning applications.\n",
    "The dataset is a CSV file, named \"spam.csv\", containing text messages classified as ham or spam, with a total of 5572 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0:  Importing the necessary libraries\n",
    "\n",
    "#### Pandas\n",
    "Python Library used for data manipulation. \n",
    "It is used here to read the dataset from the csv file. [Used in cell 1]\n",
    "#### CountVectorizer\n",
    "CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. \n",
    "It is used here to create a vocabulary from the text from dataset and encode it. [Used in cell 3]\n",
    "#### SVM\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "For this classification, SVM model is imported using scikit-learn [Used in cell 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Reading the dataset\n",
    "The dataset is a simple text dataset of size (5572 x 2), ie 5572 dataset examples or rows and 2 heading or columns (EmailText and Label). \n",
    "\n",
    "This classification dataset has 2 classes :-\n",
    "(1) Ham - Genuine emails that are not spam\n",
    "(2) Spam\n",
    "\n",
    "The dataset will be saved in variable called 'data', as a Panda dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"spam.csv\")\n",
    "# print(data) \n",
    "# print(type(data)) # Pandas Dataframe\n",
    "# print(data.shape) # 5572 rows and 2 columns\n",
    "# print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Splitting the dataset\n",
    "The dataset needs to be split into x and y ie data and label. Then it is again split into training and testing dataset in the ratio of 75% training set and 25% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"EmailText\"]\n",
    "y = data[\"Label\"]\n",
    "\n",
    "### Get the number of examples\n",
    "data_size = data.shape[0]\n",
    "### Splitting ratio - 75% training set and 25% testing set\n",
    "dataset_split_percent = 0.75\n",
    "data_split = round(data_size*dataset_split_percent)\n",
    "# print(data_split)\n",
    "\n",
    "x_train, y_train = x[0:data_split], y[0:data_split]\n",
    "x_test, y_test = x[data_split:data_size], y[data_split:data_size]\n",
    "\n",
    "# print(x_train)\n",
    "# print(y_train)\n",
    "# print(x_test)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Feature Extraction\n",
    "Here, features used for classification, is a count of words used in any email. Individual words are taken from the training data, i.e. x_train and the frequency of those words is evaluated. This frequency in the occurence of words acts as the feature to classify emails.\n",
    "For this, an inbuilt function of scikit-learn is used, called CountVectorizer. It converts a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "features = cv.fit_transform(x_train)\n",
    "\n",
    "# print(features)\n",
    "# print(features.shape)\n",
    "### get_feature_names function produces a list which allows you to access the labels/features of the countvectorizer, i.e. the individual unique words in the dataset.\n",
    "# print(cv.get_feature_names())\n",
    "### length of the above mentioned list, i.e. the number of unique words in the dataset\n",
    "# print(len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Training Model\n",
    "The model used for the classification is SVM (Support Vector Machines). This is a basic SVM model with default features. The features or parameters of the model can be changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Model creation\n",
    "model = svm.SVC()\n",
    "### Model training\n",
    "model.fit(features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Testing and accuracy\n",
    "The working of the model is tested by running the testing dataset on the model and scoring the accuracy by comparing it against the labels of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9834888729361091\n"
     ]
    }
   ],
   "source": [
    "test = cv.transform(x_test)\n",
    "# print(test)\n",
    "# print(y_test)\n",
    "accuracy = model.score(test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
